---
title: "Milestone Report"
author: "Alex Perez Nava"
date: "22/9/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploratory Data Analysis and Modeling

This document presents the milestone report; it regards conducting an exploratory data analysis using the Capstone Dataset to demonstrate the following:

-Dataset is downloaded and run successfully

-Basic report of summary statistics about the data

-Report any interesting findings

-Plans for final prediction algorithm and Shiny app

## Get the data
You must download the data from the link below and placed it in your working directory:

https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

Store corpora in vectors as follows
```{r }
blog <- readLines("en_US.blogs.txt",encoding = "UTF-8",skipNul = TRUE)
news <- readLines("en_US.news.txt",encoding = "UTF-8",warn = FALSE,skipNul = TRUE)
twitter <- readLines("en_US.twitter.txt",encoding = "UTF-8",skipNul = TRUE)
## Make sure data is loaded correctly by checking one vector
typeof(blog)
length(blog)
```

## Explore data
Look at the size of the files (MegaBytes)
```{r }
file.info("~/Documents/Data_Science_Capstone/final/en_US/en_US.blogs.txt")$size/1024^2
file.info("~/Documents/Data_Science_Capstone/final/en_US/en_US.news.txt")$size/1024^2
file.info("~/Documents/Data_Science_Capstone/final/en_US/en_US.twitter.txt")$size/1024^2
```

Load Packages
```{r echo=TRUE, warning=TRUE}
library(stringi)
library(ggplot2)
library(tm)
library(wordcloud)
library(RWeka)
library(pryr)
```

Look at non-missing strings in each corpus, lines with white spaces, number of unicode code points and unicode code points that are not white spaces.

```{r }
## Blog
stri_stats_general(blog)
## News
stri_stats_general(news)
## Twitter
stri_stats_general(twitter)
```

Count words and summarise the distribution per corpus. Table shows the exploratory analysis:

1.-Lines - number of lines (number of non-missing strings in the vector);

2.-LinesNEmpty - number of lines with at least one non-WHITE_SPACE character;

3.-Chars - total number of Unicode code points detected;

4.- CharsNWhite - number of Unicode code points that are not WHITE_SPACEs;

```{r }
summarytable <- data.frame(Title=c("blog","news","twitter"),Line_Count=c(length(blog),length(news),length(twitter)), 
                           Max_Character= c(max(nchar(blog)),max(nchar(news)),max(nchar(twitter))), 
                           Total_Character=c(sum(nchar(blog)),sum(nchar(news)),sum(nchar(twitter))))
summarytable
```

Store in vectors the counting of words
```{r }
words_blog <- stri_count_words(blog)
words_news <- stri_count_words(news)
words_twitter <- stri_count_words(twitter)
```

Look at  corpora

Using "readLines" function we can look at the first five rows of the twitter corpus. This step is important to understand the structure of the data.

```{r }
content_twitter <- file("en_US.twitter.txt")
readLines(content_twitter, 5)
```

Frequency analysis shows that the average words per tweet is 13
```{r }
hist(words_twitter)
summary(words_twitter)
```

```{r}
content_blog <- file("en_US.blogs.txt")
readLines(content_blog, 5)
```

```{r }
plot(words_blog, ylim = c(0, 550))
summary(words_blog)
```

Looking at the summary, there are very large blog chunks, some have over 6 thousand words (outlier data), so for this reason and to avoid crashes, we limit the number of words to 550. You can see at the plot that density is higher bellow 250 words.

```{r }
content_news <- file("en_US.news.txt")
readLines(content_news, 5)
```

```{r }
plot(words_news, ylim = c(0, 300))
summary(words_news)
```

Regarding news, we have higher density bellow 150 words. We limited number of words to 300 as mean is 34 and 3rd Qu. is only 46 words.


## Exploratory Data Results and Findings

Regarding the size, 'en_US.blogs.txt' is the largest cospus with 200 MegaBytes and file 'en_US_twitter.txt' is the smallest with 160.

In terms of number of words, twitter corpus is the largest with over 2 million, but in contrast it has the smallest number of characters.

We could conclude that twitter users try to force the use of short words, acronyms and abbreviations because the character limit of 140 per tweet.


## Sampling and Cleaning Data
As the data size is large, we should sample it to run models on smaller dataset. Let's use 1% sample of data. 
To get a clean data, it is necessary convert lines to lower case, remove white spaces, punctuation, stop words and numbers.

```{r echo=TRUE, warning=TRUE}
set.seed(12345)
test_data <- c(sample(blog, length(blog) * 0.01),
               sample(news, length(news) * 0.01),
               sample(twitter, length(twitter) * 0.01)
)
gc()
mem_used()
```


```{r }
testdata <- iconv(test_data, "UTF-8", "ASCII", sub="")
sample_corpus <- VCorpus(VectorSource(testdata))
sample_corpus <- tm_map(sample_corpus, tolower)
sample_corpus <- tm_map(sample_corpus, stripWhitespace)
sample_corpus <- tm_map(sample_corpus, removePunctuation)
sample_corpus <- tm_map(sample_corpus, removeNumbers)
mystopwords <- c(stopwords("english"))
sample_corpus <- tm_map(sample_corpus, removeWords, mystopwords)
sample_corpus <- tm_map(sample_corpus, PlainTextDocument)
```

## Creating N-gram models for our data
We have cleaned and sampled our data to use 1% of the corpora.  Use the following functions to create basic 1-gram, 2-gram and 3-gram models.

```{r }
unigram <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
bigram <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
trigram <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
```

```{r }
unidtf <- TermDocumentMatrix(sample_corpus, control=list(tokenize=unigram))
bidtf <- TermDocumentMatrix(sample_corpus, control=list(tokenize=bigram))
tridtf <- TermDocumentMatrix(sample_corpus, control=list(tokenize=trigram))
```


```{r }
uni_tf <- findFreqTerms(unidtf, lowfreq = 50 )
bi_tf <- findFreqTerms(bidtf, lowfreq = 50 )
tri_tf <- findFreqTerms(tridtf, lowfreq = 10 )
```


```{r }
uni_freq <- rowSums(as.matrix(unidtf[uni_tf, ]))
uni_freq <- data.frame(words=names(uni_freq), frequency=uni_freq)
```


```{r }
bi_freq <- rowSums(as.matrix(bidtf[bi_tf, ]))
bi_freq <- data.frame(words=names(bi_freq), frequency=bi_freq)
```


```{r }
tri_freq <- rowSums(as.matrix(tridtf[tri_tf, ]))
tri_freq <- data.frame(words=names(tri_freq), frequency=tri_freq)
```


```{r }
head(tri_freq)
```

## Ploting N-gram modelig results
Using wordcloud package we get the frequency plot for 150 lines.
```{r }
wordcloud(words=uni_freq$words, scale=c(3,0.1), freq=uni_freq$frequency, max.words=150, colors = brewer.pal(8, "Dark2"))
```

```{r }
plot_freq <- ggplot(data = uni_freq[order(-uni_freq$frequency),][1:15, ], aes(x = reorder(words, -frequency), y=frequency)) +
  geom_bar(stat="identity", fill="green") + 
  ggtitle("Top Unigram") + xlab("words") +  ylab("frequency")
plot_freq
```

1-gram model indicates that "said" is the most frequent line.

```{r }
plot_freq2 <- ggplot(data = bi_freq[order(-bi_freq$frequency),][1:15, ], aes(x = reorder(words, -frequency), y=frequency)) +
  geom_bar(stat="identity", fill="red") + theme(axis.text.x = element_text(angle = 45)) +
  ggtitle("Top Bigram") + xlab("words") +  ylab("frequency")
plot_freq2
```

2-gram model shows that "right now" are the most frequent lines.

```{r }
plot_freq3 <- ggplot(data = tri_freq[order(-tri_freq$frequency),][1:15, ], aes(x = reorder(words, -frequency), y=frequency)) +
  geom_bar(stat="identity", fill="yellow") + theme(axis.text.x = element_text(angle = 45)) +
  ggtitle("Top Trigram") + xlab("words") +  ylab("frequency")
plot_freq3
```

Finally, 3-gram model indicates thtat "happy mothers day" are tha most frequent lines.

## Prediction Algorithm and Shiny App
We have loaded our data and done an Exploratory analysis N-gram modeling helped us to understand frequency lines analysis and plots show results.

For the next step is important data partition to avoid crashes and also to have the possibility to run de app in a simple device like a cell phone.  We will search for data relevancy going through corpora analysis and modeling to find the one that best fit for prediction based on previous words.  Special attention is needed on model accuracy and error.


Create a shiny app to predict the next word based on the high frequency rate. Frequency rate would outline the best prediction model.

